1. 回归算法-线性回归分析
    线性回归:
      目标值: 连续的
        寻找一种能预测的趋势
      二维:
        直线关系
      三维:
        特征, 目标值  平面
      线性关系定义:
        y = kx + b
        b: 偏置
          单个特征: 更加通用
          多个特征: 找到合适的k预测结果
        定义:
          特征与目标值之间进行建模的回归分析
      矩阵:
        大多数算法计算基础
        必须是二维
        满足了特定运算需求
        乘法:
          (m行, l列) * (l行, n列) = (m行, n列)
      迭代算法:
        回归
          预测时候有差距
          损失函数: (误差)
              算法      策略(损失函数)        优化    >>>迭代过程
            线性回归      误差平方和
                         最小二乘法

            逻辑回归

            目的:
              寻找最优化的w
              w的数量取决于特征的数量
            优化方法:
              正规方程
              梯度下降:
                沿着函数下降的方向找, 更新w值

2. 线性回归实例
    sklearn:
      优点:
        封装好, 建立模型简单, 预测简单
      缺点:
        算法的过程, 参数都在API内部优化
      神经网络

3. 回归性能评估
      均方误差
    梯度下降:
      需要选择学习率
      多次迭代
      n很大
      适用于各种模型
    正规方程:
      不需要
      一次得出
      n < 10000 时可以
      线性模型, 不适合与逻辑回归等其他模型
        过拟合:
          训练数据上能获得很好的拟合, 此外的数据不能很好 (模型复杂)
            eg:
              LinerRegression
            根据结果现象判断: 过拟合, 欠拟合
              欠拟合
                训练集: 不好
                测试集: 不好
              过拟合:
                训练集: 好
                测试集: 不好
          解决办法:
            特征选择
            L2 正则化
        欠拟合:
          训练数据和此外的数据都不能获得很好的拟合     (模型过于简单)
          解决办法:
            增加数据的特征数量
    线性:
      简单模型
    非线性:
      复杂模型
        原因:
          数据的特征和目标值之间的关系不仅仅是线性关系
    特征选择:
      过滤式:
        低方差特征
      嵌入式:
        决策树
        减少权重  >>> 0
        正则化:
          L2正则化:
        Ridge: (岭回归)
          alpha: 正则化力度  0~1 1~10
            网格搜索
            影响:
              力度越大, w >>> 0
          LinearRegression 与 Ridge 对比
            岭回归:
              回归系数更符合实际, 更可靠
        神经网络

4. 分类算法-逻辑回归
    逻辑回归:   (二分类)

      线性回归的式子作为逻辑回归的输入
      特点:
        得出概率值
      sigmoid函数:         (对数似然损失函数)
        特点:
          将输入的特征转换为 0~1 之间的值
      线性回归的输入 >>> 分类

    损失函数:
      均方误差         (梯度下降求解)
        不存在多个局部最低点, 只有一个最小值
      对数似然损失      (梯度下降求解)
        多个局部最小值  (方法)  >>> 尽量改善
          1) 多次随机初始化, 多次比较最小值结果
          2) 调整学习率
    正例:
      类别少的概率
    反例:
      类别大的概率
    优点:
      分类概率的场景, 简单, 速度快
    缺点:
      不好处理多分类问题

  多分类:
    softamx方法         无             先验概率: 总结历史数据概率信息
                 逻辑回归(判别模型)          朴素贝叶斯(生成模型)
        解决问题       二分类                     多分类
        应用场景    癌症, 二分类                  文本分类
          参数       正则化力度                    没有
    k-近邻, 决策树, 随机森林, 神经网络            隐马科夫模型


5. 逻辑回归实例
6. 聚类算法-kmeans
    非监督学习:
      k: 把数据划分成多少个类别
        知道类别的个数
          过程:
            假设 k=3
            1) 随机在数据当中抽取三个样本, 当做三个类别的中心点(k1, k2, k3)
            2) 计算其余的点分别到三个点的距离, 每一个样本有三个距离(a, b, c), 从中选择最近的点作为自己的标记形成三个族群
            3) 分别计算三个族群的平均值, 把三个平均值与之间的三个旧的中心点进行比较
                相同:
                  结束聚类
                不同:
                  三个平均值当做新的中心点, 重复第二步
        超参数
    轮廓系数:   (每一个样本)
      原理:   [-1, 1]
        外部距离最大化
        内部距离最小化
    特点分析:
      采用迭代算法
      缺点:
        容易收敛到局部最优解
      解决办法:
          多次聚类
7. k-means实例