day02
1. sklearn数据集与估计器
  数据集划分
    评估的模型和建立的数据不能用的一模一样
      训练集  (大多数)
        建立模型
      测试集
        评估

      占比 7 3
      占比 8 2
      占比 3/4 1/4   最佳   75%   25%
  sklearn数据集接口介绍
    sklearn.datasets
      load* fetch*  返回的数据类型 datasets.base.Bunch(字典格式)
  sklearn分类数据集
  sklearn回归数据集

  fit_transform(): 输入数据直接转换
    fit(): 输入数据, 但不转换
      eg:
        计算平均值, 方差等
    + transform(): 进行数据的交换

  estimator 估计器   (x:特征值    y:目标值)
    训练集
    测试集
      1. 调用fit
        fit(x_train, y_train)
      2. 输入与测试集的数据
        x_test, y_test
        1) y_pridict = predict(x_test)
        2) 预测的准确率: score(x_test, y_test)

2. 分类算法-k近邻算法
    判别依据:
      数据: 离散型
    核心思想:
      相似的样本, 特征之间的值应该是相近的
    距离如何求:
      1) 需要做标准化处理
      2) 欧式距离
      3) k 取值
          k 取小: 容易受异常点影响
          k 取大: 容易受k值数量波动
    优点:
      无需参数估计, 无需训练, 简单, 易于理解
    缺点:
      k 取值

    使用场景:
      小数据场景,

3. k-近邻算法实例
    特征值:
      x, y 坐标
        0 < x < 10
        0 < y < 10
      定位准确性
      时间戳
        sklearn 删除0
        pandas  删除1
    目标值:
      入住位置的id

    1) 缩小x, y
    2) 时间戳进行 (年, 月, 日, 周, 时分秒)    新的特征
    3) 几千~几万, 少于指定签到人数的位置删除

4. 分类模型的评估
    混淆矩阵
      二分类
        精确率
        召回率

5. 分类算法-朴素贝叶斯算法
    1) 概率基础
      联合概率
        P(A,B) = P(A) * P(B)
      条件概率 (A1,A2 相互独立)
        P(A1,A2|B) = P(A1|B) * P(A2|B)
    2) 朴素贝叶斯介绍  (特征之间独立)
      应用场景:
        文档分类
      贝叶斯公式
        P(C|W) = P(W|C)P(C) / P(W)
        拉普拉斯平滑系数
      缺点:
        对训练集影响非常大
      优点:
        不需要调参
        缺失数据不敏感
        准确度高

6. 模型的选择与调优
    交叉验证
      模型评估更加准确可信
        所有数据分成n等分
      训练集
      验证集

    网格搜索
      调参:
          k-近邻
            超参数k
            每组超参数采用交叉验证来进行评估
            10折交叉验证

7. 决策树与随机森林
    比特
    根据信息论(香农)
    信息熵:
      单位: 比特
      信息和不确定性是相联系的
      信息熵越大, 效果越不好
    分类依据:
      信息增益:
        当得知一个条件之后, 减少的信息熵的大小
        公式:
          g(D,A) = H(D) - H(D|A)
    常见算法:
      ID3
      C4.5
      CART
    sklearn中基尼系数
    导出决策树:
      安装插件: sudo apt-get install graphviz
        dot文件
      优点:
        简单的理解和解释, 树木可视化
        需要很少的数据准备, 其他技术通常需要数据归一化
      缺点:
        过拟合
      改进:
        减枝cart算法
    随机森林:
      建立模型组合, 生成多个分类器/模型
      包含多个决策树的分类器, 并且输出的类别是由个别树输出的类别的众数而定
      过程: (重点)
        单个树:
          1) 随机在N个样本当中选择一个样本, 重复n次  样本可能重复
          2) 随机在M个特征中选出m个特征,  m取值
        多个树:
          随机有放回的抽样 bootstrap抽样
      超参数:
        n_estimator: 决策树的数量
        max_depth: 每棵树的深度限制
      优点:
        极高的准确率
        运行在大数据集上
        高维特征的输入样本, 不需要降维
        评估各个特征在分类问题上的重要性